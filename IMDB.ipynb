{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60a0a565-1e61-4c3c-baf6-7e07ab7f09fb",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on IMDB Dataset\n",
    "#### *Hermela Seltanu Gizaw*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514731c9-ce5c-4d75-8112-bacbf7a99713",
   "metadata": {},
   "source": [
    "#### Objective: \n",
    "To perform a comparative study of different text representation techniques in sentiment analysis using traditional machine learning.\n",
    "\n",
    "#### Project Description\n",
    "This project focuses on classifying 50,000 movie reviews from the IMDB Dataset into positive or negative sentiments. The primary goal is to evaluate the efficacy of various feature extraction methods when paired with a traditional Logistic Regression classifier. To ensure the highest accuracy, a comprehensive NLP pipeline was implemented, including HTML noise removal, case normalization, stop-word filtering, and lemmatization.\n",
    "\n",
    "#### Core Tasks and Techniques:\n",
    "The model was trained and evaluated using four distinct text representation strategies:\n",
    "\n",
    "1. One-Hot Encoding: A binary representation capturing word existence.\n",
    "2. Bag of Words (BoW): A frequency-based count of word occurrences.\n",
    "3. TF-IDF (Term Frequency-Inverse Document Frequency): A statistical weighting method that penalizes common \"noise\" words and rewards distinctive sentiment indicators.\n",
    "4. Word2Vec: A prediction-based embedding technique using the Skip-gram architecture to capture semantic relationships, transformed into document vectors via mean pooling.\n",
    "\n",
    "By comparing these techniques, this project demonstrates how the mathematical representation of text significantly impacts the predictive performance and interpretability of traditional machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2983ad1-5133-4737-84ae-6c86b0fa975c",
   "metadata": {},
   "source": [
    "### Library Imports\n",
    "In this initial step, we import the necessary Python packages. We use pandas for data manipulation, re for cleaning text via regular expressions, nltk for natural language processing tasks, and sklearn for our traditional machine learning classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "329e927a-c535-4e6b-b786-845a41b4b559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported and NLTK data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import logging\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "# Complete silence for NLTK and Warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "for package in ['stopwords', 'wordnet', 'punkt']:\n",
    "    nltk.download(package, quiet=True)\n",
    "\n",
    "print(\"Libraries imported and NLTK data loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2366ab-c1c6-47b9-9275-7f500b36fd36",
   "metadata": {},
   "source": [
    "### Dataset Loading and Initial Inspection\n",
    "We load the IMDB dataset from the CSV file. A critical part of this step is encoding the sentiment labels as binary integers (0 for negative, 1 for positive) so they can be processed by our machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd999ff4-8bbe-44d8-a691-797a48ace73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (50000, 3)\n",
      "\n",
      "Sentiment Distribution:\n",
      " sentiment\n",
      "positive    25000\n",
      "negative    25000\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  label\n",
       "0  One of the other reviewers has mentioned that ...  positive      1\n",
       "1  A wonderful little production. <br /><br />The...  positive      1\n",
       "2  I thought this was a wonderful way to spend ti...  positive      1\n",
       "3  Basically there's a family where a little boy ...  negative      0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive      1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "# Ensure the dataset file is named 'IMDB Dataset.csv' in your local directory\n",
    "df = pd.read_csv('IMDB Dataset.csv')\n",
    "\n",
    "# Label Encoding\n",
    "df['label'] = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "\n",
    "# Check for class balance to ensure the training data is not biased\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nSentiment Distribution:\\n\", df['sentiment'].value_counts())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55510d5b-f49e-4efd-bf09-d7fca423cb80",
   "metadata": {},
   "source": [
    "- As shown in our initial inspection, the dataset is perfectly balanced with 25,000 samples for each class, which ensures that our model accuracy will be a reliable metric for performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed3f5d2-c271-43fc-9c9e-bfceaceb3662",
   "metadata": {},
   "source": [
    "### Text Preprocessing and Normalization\n",
    "Text data in the IMDB dataset contains HTML tags and punctuation that act as \"noise.\" We standardize the text by converting it to lowercase, removing non-alphabetical characters, and applying Lemmatization to group different forms of the same word (e.g., \"acting\" and \"act\") together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cc56a95-e2fb-417b-9a32-b84e0deb6c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of cleaned text:\n",
      " one reviewer mentioned watching oz episode hooked right exactly happened first thing struck oz brutality unflinching scene violence set right word go trust show faint hearted timid show pull punch reg\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove HTML <br /> tags\n",
    "    text = re.sub(r'<br\\s*/?>', ' ', text)\n",
    "    # Remove special characters and numbers, then lowercase\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text).lower()\n",
    "    # Tokenization\n",
    "    tokens = text.split()\n",
    "    # Remove stopwords and apply Lemmatization\n",
    "    cleaned = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
    "    return \" \".join(cleaned)\n",
    "\n",
    "# Apply preprocessing to the entire dataset\n",
    "df['cleaned_review'] = df['review'].apply(preprocess_text)\n",
    "print(\"Sample of cleaned text:\\n\", df['cleaned_review'][0][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf214e7-954f-4f10-b254-7665310de513",
   "metadata": {},
   "source": [
    "### Frequency-Based Representations (One-Hot & BoW)\n",
    "We implement the first two representations using CountVectorizer.\n",
    "\n",
    "- One-Hot Encoding sets binary=True to represent only the presence (1) or absence (0) of a word.\n",
    "- Bag of Words (BoW) sets binary=False to capture the raw frequency counts of words.\n",
    "We limit the vocabulary to 10,000 features to ensure the models remain computationally efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b86b0dff-d7f8-45de-b6d7-b463095bbcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot Matrix Shape:    (50000, 10000)\n",
      "Bag of Words Matrix Shape: (50000, 10000)\n",
      "\n",
      "--- Direct Comparison: One-Hot vs. Bag of Words ---\n",
      "Word            | BoW Count  | One-Hot Value\n",
      "---------------------------------------------\n",
      "away            | 2          | 1         \n",
      "city            | 2          | 1         \n",
      "due             | 2          | 1         \n",
      "episode         | 2          | 1         \n",
      "first           | 2          | 1         \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 1. Implementation\n",
    "ohe_vectorizer = CountVectorizer(binary=True, max_features=10000)\n",
    "X_ohe = ohe_vectorizer.fit_transform(df['cleaned_review'])\n",
    "\n",
    "bow_vectorizer = CountVectorizer(binary=False, max_features=10000)\n",
    "X_bow = bow_vectorizer.fit_transform(df['cleaned_review'])\n",
    "\n",
    "# 2. Precise Comparison Logic\n",
    "sample_index = 0\n",
    "# We use the vocabulary from the BoW vectorizer to look up indices in both\n",
    "feature_names = bow_vectorizer.get_feature_names_out()\n",
    "ohe_vocab = ohe_vectorizer.vocabulary_\n",
    "\n",
    "print(f\"One-Hot Matrix Shape:    {X_ohe.shape}\")\n",
    "print(f\"Bag of Words Matrix Shape: {X_bow.shape}\")\n",
    "print(\"\\n--- Direct Comparison: One-Hot vs. Bag of Words ---\")\n",
    "print(f\"{'Word':<15} | {'BoW Count':<10} | {'One-Hot Value':<10}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Pick words that appeared more than once in the first review\n",
    "sample_bow_row = X_bow[sample_index].toarray().flatten()\n",
    "multiple_occurrence_indices = [i for i, val in enumerate(sample_bow_row) if val > 1]\n",
    "\n",
    "for i in multiple_occurrence_indices[:5]:\n",
    "    word = feature_names[i]\n",
    "    # Find the corresponding index in the One-Hot matrix\n",
    "    ohe_idx = ohe_vocab[word]\n",
    "    bow_val = X_bow[sample_index, i]\n",
    "    ohe_val = X_ohe[sample_index, ohe_idx]\n",
    "    print(f\"{word:<15} | {int(bow_val):<10} | {int(ohe_val):<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765de20e-9abd-4b90-a5e3-68bb85e31077",
   "metadata": {},
   "source": [
    "- The direct comparison above highlights the fundamental distinction between these two traditional text representations. While both matrices share the same dimensions of 50,000 reviews by 10,000 features, the values within them serve different purposes. In the Bag of Words column, the model captures the raw frequency of terms (showing that words like \"episode\" or \"city\" appear twice), which allows the classifier to measure the \"intensity\" of a review's focus.\n",
    "\n",
    "- In contrast, the One-Hot Value column remains capped at a binary 1, demonstrating that it only records the presence of a feature regardless of its repetition. This difference is key: BoW provides a denser signal for the model to weigh repeated sentiment, while One-Hot offers a simpler, categorical existence check."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3e4c3d-bfa6-4a40-bf0d-a4fe6e1da786",
   "metadata": {},
   "source": [
    "### Statistical Weighting Representation (TF-IDF)\n",
    "While BoW treats all words equally, TF-IDF (Term Frequency-Inverse Document Frequency) uses the IDF log-function to lower the weight of words that appear too frequently across all reviews (like \"movie\" or \"film\") and increases the weight of words that are rare but sentiment-heavy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebe03ea3-fa0d-43f4-8fcc-154b2b9b1d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix Shape: (50000, 10000)\n",
      "\n",
      "Words with lowest IDF (most common/least distinctive):\n",
      "       word       idf\n",
      "5888  movie  1.433298\n",
      "3405   film  1.537566\n",
      "6233    one  1.548167\n",
      "5225   like  1.753512\n",
      "9076   time  1.900733\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Implementation 3: TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['cleaned_review'])\n",
    "\n",
    "print(f\"TF-IDF Matrix Shape: {X_tfidf.shape}\")\n",
    "\n",
    "# Highlighting the weighting difference:\n",
    "# We can look at the IDF weights to see which words were deemed \"least important\"\n",
    "word_weights = pd.DataFrame({'word': tfidf_vectorizer.get_feature_names_out(), \n",
    "                             'idf': tfidf_vectorizer.idf_})\n",
    "print(\"\\nWords with lowest IDF (most common/least distinctive):\")\n",
    "print(word_weights.sort_values(by='idf').head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4568af6e-e543-4091-a21d-3c71b77e3426",
   "metadata": {},
   "source": [
    "- The output confirms that our TF-IDF implementation is successfully filtering the dataset's \"noise.\" The matrix dimensions of (50,000, 10,000) show we are processing the full dataset while maintaining a controlled vocabulary of the top 10,000 features to prevent overfitting. Most importantly, the words with the lowest Inverse Document Frequency (IDF) scores; such as \"movie\" (1.43) and \"film\" (1.53) - reveal the model's intelligence; it has correctly identified that because these terms appear in almost every review, they carry no \"weight\" for distinguishing between positive and negative sentiment.\n",
    "\n",
    "- By mathematically penalizing these common terms, TF-IDF forces the Logistic Regression classifier to prioritize rare, high-impact adjectives that actually define sentiment, which is why this technique achieved our highest accuracy of 89.35%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ebc36f-ce08-416d-96ff-2bb3463f2b72",
   "metadata": {},
   "source": [
    "### Semantic Embedding Representation (Word2Vec)\n",
    "For the final representation, we train a Word2Vec model. We use the Skip-gram (sg=1) architecture. To convert word vectors into a Document Embedding, we compute the average of all word vectors in each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00787784-ef9e-416f-9c7c-aafcada539a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Feature Matrix Shape: (50000, 100)\n",
      "\n",
      "--- Insight: Semantic Similarity Learned by Word2Vec ---\n",
      "Words most similar to 'excellent': ['superb', 'outstanding', 'terrific']\n",
      "Words most similar to 'bad':       ['terrible', 'awful', 'horrible']\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# 1. Training\n",
    "sentences = [doc.split() for doc in df['cleaned_review']]\n",
    "w2v_model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=5, sg=1)\n",
    "\n",
    "# 2. Vectorization\n",
    "def get_doc_vector(tokens, model):\n",
    "    valid_vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if not valid_vectors:\n",
    "        return np.zeros(100)\n",
    "    return np.mean(valid_vectors, axis=0)\n",
    "\n",
    "X_w2v = np.array([get_doc_vector(s, w2v_model) for s in sentences])\n",
    "\n",
    "# 3. Enhanced Output: Demonstrate Semantic Understanding\n",
    "print(f\"Word2Vec Feature Matrix Shape: {X_w2v.shape}\")\n",
    "print(\"\\n--- Insight: Semantic Similarity Learned by Word2Vec ---\")\n",
    "print(\"Words most similar to 'excellent':\", [w for w, s in w2v_model.wv.most_similar('excellent', topn=3)])\n",
    "print(\"Words most similar to 'bad':      \", [w for w, s in w2v_model.wv.most_similar('bad', topn=3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fddfa8-3c01-4b66-ae0d-3814cc3b4e86",
   "metadata": {},
   "source": [
    "- The Word2Vec output confirms that the model has successfully captured the Distributional Hypothesis, which posits that words occurring in similar contexts share similar meanings. The matrix shape of (50,000, 100) indicates that each of our 50,000 reviews has been compressed into a dense, 100-dimensional vector. Unlike frequency-based methods, the \"Semantic Similarity\" results prove that the model understands linguistic nuances; for instance, it correctly identifies that \"excellent\" is semantically related to \"superb\" and \"outstanding\".\n",
    "\n",
    "- This is a powerful advantage for sentiment analysis, as it allows the classifier to generalize its learning across synonyms, ensuring that even if a specific word appears rarely, its semantic \"neighborhood\" can guide the model toward the correct sentiment prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f080270-9237-4a15-90d7-ccdbe913472e",
   "metadata": {},
   "source": [
    "### Model Training and Comparative Evaluation\n",
    "In this step, we train a Logistic Regression classifier for each of our four text representations. To ensure a fair \"apples-to-apples\" comparison, we use the same model architecture and the same train-test split for every iteration. We store the results in a dictionary to generate a final performance table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "458d4c13-9ee0-4c38-8945-ca2a2a2414dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished evaluating: One-Hot Encoding\n",
      "Finished evaluating: Bag of Words\n",
      "Finished evaluating: TF-IDF\n",
      "Finished evaluating: Word2Vec (Averaged)\n",
      "\n",
      "--- Final Performance Comparison ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Representation</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.8935</td>\n",
       "      <td>0.893862</td>\n",
       "      <td>0.8935</td>\n",
       "      <td>0.893461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>0.8743</td>\n",
       "      <td>0.874299</td>\n",
       "      <td>0.8743</td>\n",
       "      <td>0.874299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Word2Vec (Averaged)</td>\n",
       "      <td>0.8729</td>\n",
       "      <td>0.872917</td>\n",
       "      <td>0.8729</td>\n",
       "      <td>0.872894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One-Hot Encoding</td>\n",
       "      <td>0.8702</td>\n",
       "      <td>0.870227</td>\n",
       "      <td>0.8702</td>\n",
       "      <td>0.870192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Representation  Accuracy  Precision  Recall  F1-Score\n",
       "2               TF-IDF    0.8935   0.893862  0.8935  0.893461\n",
       "1         Bag of Words    0.8743   0.874299  0.8743  0.874299\n",
       "3  Word2Vec (Averaged)    0.8729   0.872917  0.8729  0.872894\n",
       "0     One-Hot Encoding    0.8702   0.870227  0.8702  0.870192"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Dictionary to hold the final metrics\n",
    "comparison_data = []\n",
    "\n",
    "def evaluate_technique(X, y, name):\n",
    "    # Split the data consistently\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Initialize and train the Traditional ML model\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average='weighted')\n",
    "    rec = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Append results to our list\n",
    "    comparison_data.append({\n",
    "        \"Representation\": name,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Precision\": prec,\n",
    "        \"Recall\": rec,\n",
    "        \"F1-Score\": f1\n",
    "    })\n",
    "    \n",
    "    print(f\"Finished evaluating: {name}\")\n",
    "\n",
    "# Execute evaluations\n",
    "evaluate_technique(X_ohe, df['label'], \"One-Hot Encoding\")\n",
    "evaluate_technique(X_bow, df['label'], \"Bag of Words\")\n",
    "evaluate_technique(X_tfidf, df['label'], \"TF-IDF\")\n",
    "evaluate_technique(X_w2v, df['label'], \"Word2Vec (Averaged)\")\n",
    "\n",
    "# Display the Final Comparison Table\n",
    "summary_table = pd.DataFrame(comparison_data)\n",
    "print(\"\\n--- Final Performance Comparison ---\")\n",
    "display(summary_table.sort_values(by='Accuracy', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bcc835-c4d8-4ccc-962e-e4eaf0585fdb",
   "metadata": {},
   "source": [
    "### Observations and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76455074-2659-406e-b532-0a1cd4ef9d00",
   "metadata": {},
   "source": [
    "- The final performance comparison reveals that TF-IDF is the superior text representation for this task, achieving the highest accuracy of 89.35%. This success stems from its ability to mathematically down-weight generic \"noise\" words like \"movie\" and \"film\" while emphasizing unique, sentiment-rich adjectives. Bag of Words (87.43%) performed slightly better than One-Hot Encoding (87.02%), proving that word frequency (how many times a user says \"excellent\") provides a marginal boost in confidence over simple word existence.\n",
    "\n",
    "- Interestingly, Word2Vec (87.29%), while semantically advanced-did not outperform TF-IDF, likely because the \"Mean Pooling\" approach of averaging word vectors can \"blur\" specific emotional triggers in longer movie reviews compared to the precise keyword weighting of statistical methods.\n",
    "\n",
    "- In conclusion, this assignment demonstrates that traditional machine learning models like Logistic Regression remain highly effective and interpretable for sentiment analysis when paired with robust feature engineering. While One-Hot and Bag of Words offer simple baselines, the statistical refinement of TF-IDF provides the most reliable signal for identifying positive and negative reviews in the IMDB dataset. This workflow proves that understanding the mathematical relationship between terms and documents is just as critical as the choice of the classifier itself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
